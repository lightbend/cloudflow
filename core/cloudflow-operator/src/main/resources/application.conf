cloudflow {
  platform {
    cluster-type = ${?CLUSTER_TYPE}

    release-version = "Not provided"
    release-version = ${?RELEASE_VERSION}

    pod-name = "cloudflow-operator"
    pod-name = ${?POD_NAME}

    pod-namespace = "cloudflow"
    pod-namespace = ${?POD_NAMESPACE}

    api {
      # API version for client compatibility checks
      compatibility-version = "1"

      bind-interface = "0.0.0.0"
      bind-port = 5001
    }

    kafka {
      bootstrap-servers = "localhost:9092"
      bootstrap-servers = ${?KAFKA_BOOTSTRAP_SERVERS}

      strimzi-cluster-name = ${?KAFKA_STRIMZI_CLUSTER_NAME}
      strimzi-topic-operator-namespace = ${?KAFKA_STRIMZI_TOPIC_OPERATOR_NAMESPACE}

      partitions-per-topic = 53
      partitions-per-topic = ${?KAFKA_PARTITIONS_PER_TOPIC}

      replication-factor = 2
      replication-factor = ${?KAFKA_REPLICATION_FACTOR}
    }

    deployment {
      # Specifies the duration after which the platform will respond with a timeout
      # while waiting for k8s to respond.
      # a duration between responses from k8s (create / update / watch resources) longer than specified timeout will
      # result in the server responding with an internal server error.
      timeout = 5 minutes
      timeout = ${?DEPLOYMENT_TIMEOUT}

      # Resource limits for runner pods
      # NOTE: do not confuse 'm' with 'M' between cpu and memory.
      # NOTE: m=milli, M=mega (I know, it sounds so obvious!)
      # TODO: Settings should check that memory sizes are larger than (at least) a few MB
      # TODO: Setting memory to very low quantity will make the pod hang in container creating state (FailedCreatePodSandBox)

      akka-runner {
        requests-memory = "128M"
        requests-memory = ${?AKKA_RUNNER_REQUESTS_MEMORY}
        requests-cpu = "0.1"
        requests-cpu = ${?AKKA_RUNNER_REQUESTS_CPU}

        # limits is optional
        limits-memory = ${?AKKA_RUNNER_LIMITS_MEMORY}

        # limits is optional
        limits-cpu = ${?AKKA_RUNNER_LIMITS_CPU}

        # WORKAROUND FOR MEMORY ISSUES WITH RESIDENTIAL VS HEAP:
        # Max heap must be set to 50% of limits-memory, OpenJDK 8u191 backports JDK 10 cgroup support flags and enables
        # container support by default. The RamPercentage flags only apply to JVM heap.
        #   -XX:+UseContainerSupport
        #   -XX:InitialRAMPercentage
        #   -XX:MaxRAMPercentage
        #   -XX:MinRAMPercentage
        # https://www.oracle.com/technetwork/java/javase/10-relnote-issues-4108729.html
        #
        # Java NIO types will cache in a thread local map BufferCache entry that contains DirectByteBuffer references
        # (off-heap allocations). The caches are unconstrained by default. The -Djdk.nio.maxCachedBufferSize=1048576
        # system property will set a 1MB buffer per thread.
        # https://bugs.openjdk.java.net/browse/JDK-8147468
        java-opts = "-XX:MaxRAMPercentage=50.0 -Djdk.nio.maxCachedBufferSize=1048576"

        java-opts = ${?AKKA_RUNNER_JAVA_OPTS}
      }

      spark-runner-driver {
        requests-memory = "1024M"
        requests-memory = ${?SPARK_DRIVER_REQUESTS_MEMORY}

        # memory-overhead is the amount of off-heap memory to allocate in cluster mode, in MiB unless otherwise specified.
        # Optional.
        memory-overhead = ${?SPARK_DRIVER_MEMORY_OVERHEAD}

        requests-cpu = 0.5
        requests-cpu = ${?SPARK_DRIVER_REQUESTS_CPU}

        # limits-cpu is optional
        limits-cpu = ${?SPARK_DRIVER_LIMITS_CPU}

        # Do not specify max heap, this is calculated by memory-overhead
        java-opts = ""
        java-opts = ${?SPARK_DRIVER_JAVA_OPTS}
      }

      spark-runner-executor {
        requests-memory = "2G"
        requests-memory = ${?SPARK_EXECUTOR_REQUESTS_MEMORY}

        # memory-overhead is the amount of off-heap memory to allocate in cluster mode, in MiB unless otherwise specified.
        # Optional.
        memory-overhead = ${?SPARK_EXECUTOR_MEMORY_OVERHEAD}

        requests-cpu = 1
        requests-cpu = ${?SPARK_EXECUTOR_REQUESTS_CPU}

        # limits-cpu is optional
        limits-cpu = ${?SPARK_EXECUTOR_LIMITS_CPU}

        # Do not specify max heap, this is calculated by memory-overhead
        java-opts = ""
        java-opts = ${?SPARK_EXECUTOR_JAVA_OPTS}
      }

      flink-runner {
        # no of Task Managers = ceil(parallelism / no of task slots)
        parallelism = 4
        jobmanager {
          replicas = 1

          requests-memory = "512Mi"
          requests-memory = ${?FLINK_JOBMANAGER_REQUESTS_MEMORY}

          limits-memory = "1024Mi"
          limits-memory = ${?FLINK_JOBMANAGER_LIMITS_MEMORY}

          requests-cpu = "0.5"
          requests-cpu = ${?FLINK_JOBMANAGER_REQUESTS_CPU}

          limits-cpu = "2.0"
          limits-cpu = ${?FLINK_JOBMANAGER_LIMITS_CPU}

          java-opts = ""
          java-opts = ${?FLINK_JOBMANAGER_JAVA_OPTS}
        }

        taskmanager {
          # Execution resources in Flink are defined through Task Slots.
          # Each TaskManager will have one or more task slots, each of which can run one pipeline of parallel tasks
          # https://ci.apache.org/projects/flink/flink-docs-stable/internals/job_scheduling.html

          # per task manager pod
          task-slots = 2

          # total across all task manager pods
          requests-memory = "1024Mi"
          requests-memory = ${?FLINK_TASKMANAGER_REQUESTS_MEMORY}

          # total across all task manager pods
          limits-memory = "2048Mi"
          limits-memory = ${?FLINK_TASKMANAGER_MEMORY_OVERHEAD}

          # total across all task manager pods
          requests-cpu = "1.0"
          requests-cpu = ${?FLINK_TASKMANAGER_REQUESTS_CPU}

          # total across all task manager pods
          limits-cpu = "2.0"
          limits-cpu = ${?FLINK_TASKMANAGER_LIMITS_CPU}

          java-opts = ""
          java-opts = ${?FLINK_TASKMANAGER_JAVA_OPTS}
        }
      }

      persistent-storage {
        resources {
          request = "15G"
          limit = "30G"
        }
        storageClassName = "nfs-client"
        storageClassName = ${?PERSISTENT_STORAGE_CLASS}
      }
    }
  }
}

akka.http.client.idle-timeout = infinite
akka.http.client.parsing.max-content-length = 64m
akka.http.client.parsing.max-to-strict-bytes = 64m
akka.http.client.parsing.max-chunk-size = 16m
akka.http.host-connection-pool.client.parsing.max-content-length = 64m
akka.http.host-connection-pool.client.parsing.max-to-strict-bytes = 64m

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
}

skuber {
  watch-continuously {
    # Timeout that is passed to the kubernetes cluster for all list/watch calls. This limits the duration of the call,
    # regardless of any activity or inactivity.
    request-timeout = 1 minute

    # The idle timeout for the connection before if closes due to inactivity. The idle-timeout must be a great value
    # than that used for timeout-seconds.
    idle-timeout = 5 minutes

    # The idle timeout for the connection pool used by the Watch Source (each source has its own connection pool).
    # When the pool is no longer used by the source and the idle time has been exceeded the pool will shutdown and
    # reclaim the unused resources.
    pool-idle-timeout = 5 minutes
  }
}
