cloudflow {
  # See https://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html
  # Rate limit on maximum number of offsets processed per trigger interval. 
  # The specified total number of offsets will be proportionally split across topicPartitions of different volume.
  # The default is set to a conservative number to prevent Out of Memory exceptions when stream processing
  # is far behind on available data in Kafka.
  spark.read.options.max-offsets-per-trigger = 500000 
  spark.read.options.max-offsets-per-trigger = ${?CLOUDFLOW_SPARK_READ_OPTIONS_MAX_OFFSETS_PER_TRIGGER}
}

# Turn off stack dumps produced by reactive kafka
akka.kafka.consumer.wakeup-debug = false
