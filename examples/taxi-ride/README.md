## Flink based Cloudflow Application

### Problem Definition

We work with two data streams, one with `TaxiRide` events generated by a Akka stream streamlet (ingress) and the other with `TaxiFare` events generated by another Akka stream streamlet (ingress). The 2 streams are then connected together through a Flink streamlet based processor which does a stateful enrichment that builds up an aggregate of `TaxiRide` to `TaxiFare` mappings.

The mapping is then posted on to an Akka stream streamlet (egress) as a tuple.

### Sub projects

The following sub-projects constitute the whole application:

* `datamodel` - contains the Avro schema for `TaxiRide`, `TaxiFare` and `TaxiRideFare`
* `ingestor` - contains the Akka stream ingresses that read data streams from http
* `processor` - the Flink streamlet that connects the input streams and does stateful processing to generate the output stream
* `logger` - contains the Akka stream egress that writes to Kafka. The logger streamlet has the following configurable parameters:
  * `valid-logger.log-level` - Log level for `*-logger` streamlets to log to. e.g. `info`
  * `valid-logger.msg-prefix` - Log line prefix for `*-logger` streamlets to include. e.g. `VALID`
* `taxi-ride-pipeline` - the entry point containing the blueprint definition

### Build the application

Here's the sequence of steps that you need to follow:

```
$ pwd
.../flink-taxi-ride
$ sbt
$ clean
$ buildAndPublish
```

The above will build the application and publish application Docker images to the Docker registry, as configured in `target-env.sbt`.

> **Note:** You need to copy `target-env.sbt.example` to `target-env.sbt` with appropriate settings for the Docker registry in order for the build and publish to go through.

The `buildAndPublish` command, if successful, will publish the exact command to use for deployment in the cluster.

### Feeding data into the application

The project comes with scripts that can be used to feed data into the ingresses using http.

The folder `test-data` contains 2 bash scripts, `send-data-rides.sh` and `send-data-fares.sh` that can be used to feed data through http to the 2 ingresses. In order to access the ingresses use port-forwarding to the two pods that are running the ingresses.
For example list the application pods:

sh```
$ kubectl get pods -n taxi-ride-fare
NAME                                                    READY   STATUS    RESTARTS   AGE
taxi-ride-fare-logger-5b5d8786bc-4qpjj                  1/1     Running   0          16m
taxi-ride-fare-processor-53e8cece-jm-7bddc97f6f-nb7km   1/1     Running   0          16m
taxi-ride-fare-processor-53e8cece-tm-bc9df878f-mqljf    1/1     Running   0          16m
taxi-ride-fare-processor-53e8cece-tm-bc9df878f-qnbsz    1/1     Running   0          16m
taxi-ride-fare-taxi-fare-8474457d5-tptr7                1/1     Running   0          16m
taxi-ride-fare-taxi-ride-84b454c56f-hr9h4               1/1     Running   0          16m
```
Then port-forward to the correct pods:
sh```
$ kubectl port-forward taxi-ride-fare-taxi-ride-84b454c56f-hr9h4 -n taxi-ride-fare 3000:3000
$ kubectl port-forward taxi-ride-fare-taxi-ride-84b454c56f-hr9h4 -n taxi-ride-fare 3001:3001
```
Now you are ready to run the two scripts.
If you want to access the Flink job manager you can check which processor pod exposes the 8080 port and also
port-forward to it.

### Example Deployment example on GKE

Steps:

1) Make sure you have installed a gke cluster and you are running Cloudflow
(check https://github.com/lightbend/cloudflow-installer for more).
Make sure you have access to your cluster:

```
$ gcloud container clusters get-credentials <CLUSTER_NAME>
```

and that you have access to the Google docker registry:

```
$ gcloud auth configure-docker
```

2) Add the Google docker registry to your sbt project (should be adjusted to your setup). Eg.

```
ThisBuild / cloudflowDockerRegistry := Some("eu.gcr.io")
ThisBuild / cloudflowDockerRepository := Some("my-awesome-project")
```

3) Build the application.

```
$ sbt buildAndPublish
```
At the very end you should see the application image built and instructions for how to deploy it:

```
[info] Successfully built and published the following Cloudflow application image:
[info]  
[info]   eu.gcr.io/my-awesome-project/taxi-ride-fare:34-69082eb-dirty
[info]  
[info] You can deploy the application to a Kubernetes cluster using any of the the following commands:
[info]  
[info]   kubectl cloudflow deploy eu.gcr.io/my-awesome-project/taxi-ride-fare:34-69082eb-dirty
[info]  
[success] Total time: 63 s, completed Nov 8, 2019 1:05:02 PM
```

4) Make sure you have the kubectl cloudflow plugin setup.

```
$ kubectl cloudflow help
This command line tool can be used to deploy and operate Cloudflow applications.
...
```
5) Deploy the app.

```
$ kubectl cloudflow deploy -u oauth2accesstoken eu.gcr.io/my-awesome-project/call-record-aggregator:34-69082eb-dirty -p
Existing value will be used for configuration parameter 'logger.log-level'
Existing value will be used for configuration parameter 'logger.msg-prefix'
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
[Done] Deployment of application `taxi-ride-fare` has started.
```

6) Verify that the application is deployed.
```
$ kubectl cloudflow list

NAME              NAMESPACE         VERSION           CREATION-TIME     
taxi-ride-fare    taxi-ride-fare    34-69082eb-dirty  2019-11-08 15:53:50 +0000 UTC
```

7) Check all pods are running.


```
$ kubectl get pods -n taxi-ride-fare
NAME                                                    READY   STATUS    RESTARTS   AGE
taxi-ride-fare-logger-7c64c57885-rmc8n                  1/1     Running   0          2m35s
taxi-ride-fare-processor-304610b6-jm-595cdf956d-cpjgh   1/1     Running   0          2m35s
taxi-ride-fare-processor-304610b6-tm-69dd56bb84-dfnjz   1/1     Running   0          2m35s
taxi-ride-fare-processor-304610b6-tm-69dd56bb84-mq7wg   1/1     Running   0          2m34s
taxi-ride-fare-taxi-fare-688ffcb66c-6jnkb               1/1     Running   0          2m35s
taxi-ride-fare-taxi-ride-7b65bb666c-fg6l4               1/1     Running   0          2m35s
```

8)Verify the application output.

Execute the steps above for ingesting data.
If everything worked fine you should see output similar to this:
sh```$ taxi-ride-fare-logger-7c64c57885-rmc8n  -n taxi-ride-fare
...
[INFO] [11/08/2019 13:35:47.510] [akka_streamlet-akka.actor.default-dispatcher-2] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 1, "totalFare": 21.5}
[INFO] [11/08/2019 13:35:47.596] [akka_streamlet-akka.actor.default-dispatcher-2] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 23485, "totalFare": 14.0}
[INFO] [11/08/2019 13:35:47.898] [akka_streamlet-akka.actor.default-dispatcher-6] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 33523, "totalFare": 14.25}
[INFO] [11/08/2019 13:35:48.568] [akka_streamlet-akka.actor.default-dispatcher-2] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 41145, "totalFare": 8.5}
[INFO] [11/08/2019 13:35:48.758] [akka_streamlet-akka.actor.default-dispatcher-6] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 44343, "totalFare": 8.0}
...
```

9) Undeploy:

```
kubectl cloudflow  undeploy taxi-ride-fare
```
