## Akka and Flink based Cloudflow Application

### Problem Definition

We work with two data streams, one with `TaxiRide` events generated by a Akka stream streamlet (ingress) and the other with `TaxiFare` events generated by another Akka stream streamlet (ingress). The 2 streams are then connected together through a Flink streamlet based processor which does a stateful enrichment that builds up an aggregate of `TaxiRide` to `TaxiFare` mappings.

The mapping is then posted on to an Akka stream streamlet (egress) as a tuple.

### Sub projects

The following sub-projects constitute the whole application:

* `datamodel` - contains the Avro schema for `TaxiRide`, `TaxiFare` and `TaxiRideFare`
* `ingestor` - contains the Akka stream ingresses that read data streams from http
* `processor` - the Flink streamlet that connects the input streams and does stateful processing to generate the output stream
* `logger` - contains the Akka stream egress that writes to Kafka. The logger streamlet has the following configurable parameters:
  * `valid-logger.log-level` - Log level for `*-logger` streamlets to log to. e.g. `info`
  * `valid-logger.msg-prefix` - Log line prefix for `*-logger` streamlets to include. e.g. `VALID`
* `taxi-ride-pipeline` - the entry point containing the blueprint definition

### Build the application

Here's the sequence of steps that you need to follow:

```
$ pwd
.../taxi-ride
$ sbt
$ clean
$ buildApp
```

The above command builds the application and publish application Docker images to the Docker registry, as configured in `target-env.sbt`.

> **Note:** You need to copy `target-env.sbt.example` to `target-env.sbt` with appropriate settings for the Docker registry in order for the build and publish to go through.

The `buildApp` command, if successful, will publish the exact command to use for deployment in the cluster.

### Feeding data into the application

By default this project is setup with a generator, so you don't have to feed data into the application. If you do want to test the HTTP ingresses, please change the blueprint and use the `taxiride.ingestor.TaxiRideIngress` and `taxiride.ingestor.TaxiFareIngress` instead of the `taxiride.ingestor.Generator`, as shown below:

```
blueprint {
  streamlets {
    rides = taxiride.ingestor.TaxiRideIngress
    fares = taxiride.ingestor.TaxiFareIngress
    processor = taxiride.processor.TaxiRideProcessor
    logger = taxiride.logger.FarePerRideLogger
  }
  topics {
    rides {
      producers = [rides.out]
      consumers = [processor.in-taxiride]
    }
    fares {
      producers = [fares.out]
      consumers = [processor.in-taxifare]
    }
    taxifares {
      producers = [processor.out]
      consumers = [logger.in]
    }
  }
}
```

The project comes with scripts that can be used to feed data into the ingresses using http.

The folder `test-data` contains 2 bash scripts, `send-data-rides.sh` and `send-data-fares.sh` that can be used to feed data through http to the 2 ingresses. Both scripts accept a port number as the argument, which you need to supply when invoking the script (see example below). In order to access the ingresses use port-forwarding to the two pods that are running the ingresses.

For example list the application pods:

```bash
$ kubectl get pods -n taxi-ride-fare
NAME                                                    READY   STATUS    RESTARTS   AGE
taxi-ride-fare-logger-5b5d8786bc-4qpjj                  1/1     Running   0          16m
taxi-ride-fare-processor-53e8cece-jm-7bddc97f6f-nb7km   1/1     Running   0          16m
taxi-ride-fare-processor-53e8cece-tm-bc9df878f-mqljf    1/1     Running   0          16m
taxi-ride-fare-processor-53e8cece-tm-bc9df878f-qnbsz    1/1     Running   0          16m
taxi-ride-fare-taxi-fare-8474457d5-tptr7                1/1     Running   0          16m
taxi-ride-fare-taxi-ride-84b454c56f-hr9h4               1/1     Running   0          16m
```

Then port-forward to the correct pods:

```bash
$ kubectl port-forward taxi-ride-fare-taxi-ride-84b454c56f-hr9h4 -n taxi-ride-fare 3000:3000
$ kubectl port-forward taxi-ride-fare-taxi-fare-8474457d5-tptr7 -n taxi-ride-fare 3001:3000
```

> Note every streamlet now exposes port 3000 only

Now you are ready to run the two scripts. Based on the above port forwards, you need to run `send-data-rides.sh` with port no `3000` and `send-data-fares.sh` with port no `3001`.

```bash
$ ./send-data-rides.sh -p 3000
...
$ ./send-data-fares.sh -p 3001
```

> **Note:** Just make sure that the port numbers passed in to the scripts match the ones where you port forwarded to.

> **Note:** If you want to access the Flink job manager you can check which processor pod exposes the 8080 port and also port-forward to it.

### Example Deployment on GKE

**Steps:**

* Make sure you have installed a GKE cluster with Cloudflow running as per the [installation guide](https://github.com/lightbend/cloudflow/blob/master/installer/README.md).
Make sure you have access to your cluster:

```bash
$ gcloud container clusters get-credentials <CLUSTER_NAME>
```

and that you have access to the Google docker registry:

```bash
$ gcloud auth configure-docker
```

* Add the Google docker registry to your sbt project (should be adjusted to your setup). The following lines should be there in the file `target-env.sbt` at the root of your application. e.g.


```
ThisBuild / cloudflowDockerRegistry := Some("eu.gcr.io")
ThisBuild / cloudflowDockerRepository := Some("my-awesome-project")
```

`my-awesome-project` refers to the project ID of your Google Cloud Platform project.

* Build the application.

```bash
$ sbt buildApp
```
At the very end you should see the application image built and instructions for how to deploy it:

```
[info] Successfully built and published the following image:
[info]   docker.io/lightbend/ingestor:467-26acd87-dirty
[info] b99104733869: Pushed
[info] 467-26acd87-dirty: digest: sha256:285ef24a568adfb6ceb738ef8ef9f5945c1885160d6870c72b64d2cec95528a9 size: 2415
[info]  
[info] Successfully built and published the following image:
[info]   docker.io/lightbend/logger:467-26acd87-dirty
[info] 87d027324c50: Pushed
[info] 467-26acd87-dirty: digest: sha256:3c1a3dd7e29305941410b14ed783a562a5a43806110817d3b6fe73e0c697f7b5 size: 3046
[info]  
[info] Successfully built and published the following image:
[info]   docker.io/lightbend/processor:467-26acd87-dirty
[success] Cloudflow application CR generated in /Users/myuser/lightbend-repos/cloudflow/examples/taxi-ride/target/taxi-ride-fare.json
[success] Use the following command to deploy the Cloudflow application:
[success] kubectl cloudflow deploy /Users/myuser/lightbend-repos/cloudflow/examples/taxi-ride/target/taxi-ride-fare.json
[success] Total time: 167 s (02:47), completed Jun 16, 2020 9:25:56 AM
```

* Make sure you have the `kubectl cloudflow` plugin configured.

```bash
$ kubectl cloudflow help
This command line tool can be used to deploy and operate Cloudflow applications.
...
```

* Deploy the app using the command mentioned in the output above:

```bash
$ kubectl cloudflow deploy /Users/myuser/lightbend-repos/cloudflow/examples/taxi-ride/target/taxi-ride-fare.json
[Done] Deployment of application `taxi-ride-fare` has started.
```

* Verify that the application is deployed.

```bash
$ kubectl cloudflow list

NAME              NAMESPACE         VERSION           CREATION-TIME     
taxi-ride-fare    taxi-ride-fare    34-69082eb-dirty  2019-11-08 15:53:50 +0000 UTC
```

* Check all pods are running.


```bash
$ kubectl get pods -n taxi-ride-fare
NAME                                                    READY   STATUS    RESTARTS   AGE
taxi-ride-fare-logger-7c64c57885-rmc8n                  1/1     Running   0          2m35s
taxi-ride-fare-processor-304610b6-jm-595cdf956d-cpjgh   1/1     Running   0          2m35s
taxi-ride-fare-processor-304610b6-tm-69dd56bb84-dfnjz   1/1     Running   0          2m35s
taxi-ride-fare-processor-304610b6-tm-69dd56bb84-mq7wg   1/1     Running   0          2m34s
taxi-ride-fare-taxi-fare-688ffcb66c-6jnkb               1/1     Running   0          2m35s
taxi-ride-fare-taxi-ride-7b65bb666c-fg6l4               1/1     Running   0          2m35s
```

* Verify the application output.

Execute the steps above for ingesting data.
If everything worked fine you should see output similar to this:

```bash
$ kubectl logs taxi-ride-fare-logger-7c64c57885-rmc8n -n taxi-ride-fare
...
[INFO] [11/08/2019 13:35:47.510] [akka_streamlet-akka.actor.default-dispatcher-2] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 1, "totalFare": 21.5}
[INFO] [11/08/2019 13:35:47.596] [akka_streamlet-akka.actor.default-dispatcher-2] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 23485, "totalFare": 14.0}
[INFO] [11/08/2019 13:35:47.898] [akka_streamlet-akka.actor.default-dispatcher-6] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 33523, "totalFare": 14.25}
[INFO] [11/08/2019 13:35:48.568] [akka_streamlet-akka.actor.default-dispatcher-2] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 41145, "totalFare": 8.5}
[INFO] [11/08/2019 13:35:48.758] [akka_streamlet-akka.actor.default-dispatcher-6] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 44343, "totalFare": 8.0}
...
```

* Undeploy.

```bash
$ kubectl cloudflow undeploy taxi-ride-fare
```

### Running inside the Sandbox

For running this application in the local sandbox, follow the instructions in [Running in a local Sandbox](https://cloudflow.io/docs/current/get-started/run-in-sandbox.html) document. Once the streamlets start following the `runLocal` command, you will see something like the following displayed on stdout:

```
logger [taxiride.logger.FarePerRideLogger]
processor [taxiride.processor.TaxiRideProcessor]
taxi-fare [taxiride.ingestor.TaxiFareIngress]
    - HTTP port [3000]
taxi-ride [taxiride.ingestor.TaxiRideIngress]
    - HTTP port [3001]
```

The port numbers indicate the ports for the two ingresses. In order to feed data to the application, you need to run the 2 bash scripts in folder `test-data`, named `send-data-rides.sh` and `send-data-fares.sh`. 

```bash
$ ./send-data-rides.sh -p 3001
...
$ ./send-data-fares.sh -p 3000
```

> **Note:** Before running the bash scripts, change the port numbers in the scripts to the ones displayed above by the `runLocal` command.

This will start feeding data to the application. You can view the output of the application in the local file that got created by the `runLocal` command.
