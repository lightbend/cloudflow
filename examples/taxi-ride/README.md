## Akka and Flink based Cloudflow Application

### Problem Definition

We work with two data streams, one with `TaxiRide` events generated by a Akka stream streamlet (ingress) and the other with `TaxiFare` events generated by another Akka stream streamlet (ingress). The 2 streams are then connected together through a Flink streamlet based processor which does a stateful enrichment that builds up an aggregate of `TaxiRide` to `TaxiFare` mappings.

The mapping is then posted on to an Akka stream streamlet (egress) as a tuple.

### Sub projects

The following sub-projects constitute the whole application:

* `datamodel` - contains the Avro schema for `TaxiRide`, `TaxiFare` and `TaxiRideFare`
* `ingestor` - contains the Akka stream ingresses that read data streams from http
* `processor` - the Flink streamlet that connects the input streams and does stateful processing to generate the output stream
* `logger` - contains the Akka stream egress that writes to Kafka. The logger streamlet has the following configurable parameters:
  * `valid-logger.log-level` - Log level for `*-logger` streamlets to log to. e.g. `info`
  * `valid-logger.msg-prefix` - Log line prefix for `*-logger` streamlets to include. e.g. `VALID`
* `taxi-ride-pipeline` - the entry point containing the blueprint definition

### Build the application

Here's the sequence of steps that you need to follow:

```
$ pwd
.../taxi-ride
$ sbt
$ clean
$ buildAndPublish
```

The above command builds the application and publish application Docker images to the Docker registry, as configured in `target-env.sbt`.

> **Note:** You need to copy `target-env.sbt.example` to `target-env.sbt` with appropriate settings for the Docker registry in order for the build and publish to go through.

The `buildAndPublish` command, if successful, will publish the exact command to use for deployment in the cluster.

### Feeding data into the application

The project comes with scripts that can be used to feed data into the ingresses using http.

The folder `test-data` contains 2 bash scripts, `send-data-rides.sh` and `send-data-fares.sh` that can be used to feed data through http to the 2 ingresses. Both scripts accept a port number as the argument, which you need to supply when invoking the script (see example below). In order to access the ingresses use port-forwarding to the two pods that are running the ingresses.

For example list the application pods:

```bash
$ kubectl get pods -n taxi-ride-fare
NAME                                                    READY   STATUS    RESTARTS   AGE
taxi-ride-fare-logger-5b5d8786bc-4qpjj                  1/1     Running   0          16m
taxi-ride-fare-processor-53e8cece-jm-7bddc97f6f-nb7km   1/1     Running   0          16m
taxi-ride-fare-processor-53e8cece-tm-bc9df878f-mqljf    1/1     Running   0          16m
taxi-ride-fare-processor-53e8cece-tm-bc9df878f-qnbsz    1/1     Running   0          16m
taxi-ride-fare-taxi-fare-8474457d5-tptr7                1/1     Running   0          16m
taxi-ride-fare-taxi-ride-84b454c56f-hr9h4               1/1     Running   0          16m
```

Then port-forward to the correct pods:

```bash
$ kubectl port-forward taxi-ride-fare-taxi-ride-84b454c56f-hr9h4 -n taxi-ride-fare 3000:3000
$ kubectl port-forward taxi-ride-fare-taxi-fare-8474457d5-tptr7 -n taxi-ride-fare 3001:3001
```

Now you are ready to run the two scripts. Based on the above port forwards, you need to run `send-data-rides.sh` with port no `3000` and `send-data-fares.sh` with port no `3001`.

```
$ ./send-data-rides.sh -p 3000
...
$ ./send-data-fares.sh -p 3001
```

> **Note:** Just make sure that the port numbers passed in to the scripts match the ones where you port forwarded to.

> **Note:** If you want to access the Flink job manager you can check which processor pod exposes the 8080 port and also port-forward to it.

### Example Deployment on GKE

**Steps:**

* Make sure you have installed a GKE cluster with Cloudflow running as per the [installation guide](https://github.com/lightbend/cloudflow/blob/master/installer/README.md).
Make sure you have access to your cluster:

```
$ gcloud container clusters get-credentials <CLUSTER_NAME>
```

and that you have access to the Google docker registry:

```
$ gcloud auth configure-docker
```

* Add the Google docker registry to your sbt project (should be adjusted to your setup). The following lines should be there in the file `target-env.sbt` at the root of your application. e.g.


```
ThisBuild / cloudflowDockerRegistry := Some("eu.gcr.io")
ThisBuild / cloudflowDockerRepository := Some("my-awesome-project")
```

`my-awesome-project` refers to the project ID of your Google Cloud Platform project.

* Build the application.

```
$ sbt buildAndPublish
```
At the very end you should see the application image built and instructions for how to deploy it:

```
[info] Successfully built and published the following Cloudflow application image:
[info]  
[info]   eu.gcr.io/my-awesome-project/taxi-ride-fare:34-69082eb-dirty
[info]  
[info] You can deploy the application to a Kubernetes cluster using any of the the following commands:
[info]  
[info]   kubectl cloudflow deploy eu.gcr.io/my-awesome-project/taxi-ride-fare:34-69082eb-dirty
[info]  
[success] Total time: 63 s, completed Nov 8, 2019 1:05:02 PM
```

* Make sure you have the `kubectl cloudflow` plugin configured.

```
$ kubectl cloudflow help
This command line tool can be used to deploy and operate Cloudflow applications.
...
```

* Deploy the app.

```
$ kubectl cloudflow deploy -u oauth2accesstoken eu.gcr.io/my-awesome-project/call-record-aggregator:34-69082eb-dirty -p "$(gcloud auth print-access-token)"
Existing value will be used for configuration parameter 'logger.log-level'
Existing value will be used for configuration parameter 'logger.msg-prefix'
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
[Done] Deployment of application `taxi-ride-fare` has started.
```

* Verify that the application is deployed.

```
$ kubectl cloudflow list

NAME              NAMESPACE         VERSION           CREATION-TIME     
taxi-ride-fare    taxi-ride-fare    34-69082eb-dirty  2019-11-08 15:53:50 +0000 UTC
```

* Check all pods are running.


```
$ kubectl get pods -n taxi-ride-fare
NAME                                                    READY   STATUS    RESTARTS   AGE
taxi-ride-fare-logger-7c64c57885-rmc8n                  1/1     Running   0          2m35s
taxi-ride-fare-processor-304610b6-jm-595cdf956d-cpjgh   1/1     Running   0          2m35s
taxi-ride-fare-processor-304610b6-tm-69dd56bb84-dfnjz   1/1     Running   0          2m35s
taxi-ride-fare-processor-304610b6-tm-69dd56bb84-mq7wg   1/1     Running   0          2m34s
taxi-ride-fare-taxi-fare-688ffcb66c-6jnkb               1/1     Running   0          2m35s
taxi-ride-fare-taxi-ride-7b65bb666c-fg6l4               1/1     Running   0          2m35s
```

* Verify the application output.

Execute the steps above for ingesting data.
If everything worked fine you should see output similar to this:

```
$ kubectl logs taxi-ride-fare-logger-7c64c57885-rmc8n -n taxi-ride-fare
...
[INFO] [11/08/2019 13:35:47.510] [akka_streamlet-akka.actor.default-dispatcher-2] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 1, "totalFare": 21.5}
[INFO] [11/08/2019 13:35:47.596] [akka_streamlet-akka.actor.default-dispatcher-2] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 23485, "totalFare": 14.0}
[INFO] [11/08/2019 13:35:47.898] [akka_streamlet-akka.actor.default-dispatcher-6] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 33523, "totalFare": 14.25}
[INFO] [11/08/2019 13:35:48.568] [akka_streamlet-akka.actor.default-dispatcher-2] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 41145, "totalFare": 8.5}
[INFO] [11/08/2019 13:35:48.758] [akka_streamlet-akka.actor.default-dispatcher-6] [akka.actor.ActorSystemImpl(akka_streamlet)] valid-logger {"rideId": 44343, "totalFare": 8.0}
...
```

* Undeploy.

```
$ kubectl cloudflow undeploy taxi-ride-fare
```

### Running inside the Sandbox

For running this application in the local sandbox, follow the instructions in [Running in a local Sandbox](https://cloudflow.io/docs/current/get-started/run-in-sandbox.html) document. Once the streamlets start following the `runLocal` command, you will see something like the following displayed on stdout:

```
logger [taxiride.logger.FarePerRideLogger]
processor [taxiride.processor.TaxiRideProcessor]
taxi-fare [taxiride.ingestor.TaxiFareIngress]
    - HTTP port [3002]
taxi-ride [taxiride.ingestor.TaxiRideIngress]
    - HTTP port [3003]
```

The port numbers indicate the ports for the two ingresses. In order to feed data to the application, you need to run the 2 bash scripts in folder `test-data`, named `send-data-rides.sh` and `send-data-fares.sh`. 

```
$ ./send-data-rides.sh -p 3003
...
$ ./send-data-fares.sh -p 3002
```

> **Note:** Before running the bash scripts, change the port numbers in the scripts to the ones displayed above by the `runLocal` command.

This will start feeding data to the application. You can view the output of the application in the local file that got created by the `runLocal` command.