= Set up example project and configure build
:toc:
:toc-title: ON THIS PAGE
:toclevels: 2

include::ROOT:partial$include.adoc[]

NOTE: Full sources for all Cloudflow example applications can be found in the
https://github.com/lightbend/cloudflow/tree/master/examples/[examples folder] of the
https://github.com/lightbend/cloudflow[`cloudflow` project on Github].
The sources for the example described below can be found in the application called
https://github.com/lightbend/cloudflow/tree/master/examples/sensor-data-scala[sensor-data-scala]
and there is also a https://github.com/lightbend/cloudflow/tree/master/examples/sensor-data-java[Java version] available.

A typical Cloudflow application uses the organization shown below. We will implement the example in Scala. Create the following directory structure:

```
   |-project
   |--cloudflow-plugins.sbt
   |-src
   |---main
   |-----avro
   |-----blueprint
   |-----resources
   |-----scala
   |-------sensordata
   |-build.sbt
```

As we move through the process, the leaf level directories of the above tree will contain the following:

* **project/cloudflow-plugins.sbt** : contains the Cloudflow `sbt` plugin name and version.
* **avro** : the avro schema of the domain objects
* **blueprint** : the blueprint of the application in a file named `blueprint.conf`
* **scala** : the source code of the application under the package name `sensordata`
* **build.sbt** : the sbt build script


== The sbt build script

Cloudflow provides sbt plugins for the supported runtimes, Akka Streams, Spark and Flink. The plugins speed development by abstracting much of the boilerplate necessary to build a complete application. You can use multiple runtimes in the same application. In this example, we use only the `CloudflowAkkaStreamsApplicationPlugin` that provides building blocks for developing a Cloudflow application with Akka Streams.

. Create a `build.sbt` file with the following contents and save it in at the same level as your `src` directory:
+
```
import sbt._
import sbt.Keys._

lazy val sensorData =  (project in file("."))
  .enablePlugins(CloudflowAkkaStreamsApplicationPlugin)
  .settings(
    libraryDependencies ++= Seq(
      "com.lightbend.akka"     %% "akka-stream-alpakka-file"  % "1.1.2",
      "com.typesafe.akka"      %% "akka-http-spray-json"      % "10.1.11",
      "ch.qos.logback"         %  "logback-classic"           % "1.2.3",
      "com.typesafe.akka"      %% "akka-http-testkit"         % "10.1.11" % "test"
    ),
    name := "sensor-data",
    organization := "com.lightbend",

    scalaVersion := "2.12.10",
    crossScalaVersions := Vector(scalaVersion.value)
  )
```
+
The script is a standard Scala sbt build file--with the addition of the Cloudflow plugin for Akka Streams.

. Add the Cloudflow plugin in `project/cloudflow-plugins.sbt`:
+
```
addSbtPlugin("com.lightbend.cloudflow" % "sbt-cloudflow" % "1.3.0")
```

NOTE: The Cloudflow sbt plugin will assume that your project is version managed using
git. It will use git commit information to generate dynamic versioning for the
Docker images it produces. If the project doesn't have an initialized git index
with at least one commit, some commands will fail with a `not a git repository`
error.

== What's next

Now, let's xref:define-avro-schema.adoc[define the Avro schema].
