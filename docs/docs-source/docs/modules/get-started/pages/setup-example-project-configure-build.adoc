= Set Up Example Project and Configure Build
:toc:
:toc-title: ON THIS PAGE
:toclevels: 2

include::ROOT:partial$include.adoc[]

NOTE: The sources for the example described below can be found in the application called
https://github.com/lightbend/cloudflow/tree/master/examples/snippets/modules/ROOT/examples/sensor-data-scala[sensor-data-scala].
Full sources for all Cloudflow example applications can be found in the
https://github.com/lightbend/cloudflow/tree/master/examples/snippets/modules/ROOT/examples[examples folder] of the
https://github.com/lightbend/cloudflow[`cloudflow` project on Github].

A typical Cloudflow application uses the organization shown below. 
We will implement the example in Scala. 

. In a convenient location, such as `my-cloudflow-example` create the following directory structure:
+
```
   |-project
   |--cloudflow-plugins.sbt
   |-src
   |---main
   |-----avro
   |-----blueprint
   |-----resources
   |-----scala
   |-------sensordata
   |-build.sbt
```
+
As we move through the process, the leaf level directories of the above tree will contain the following:
+
* **project/cloudflow-plugins.sbt** : contains the Cloudflow `sbt` plugin name and version.
* **avro** : the avro schema of the domain objects
* **blueprint** : the blueprint of the application in a file named `blueprint.conf`
* **scala** : the source code of the application under the package name `sensordata`
* **build.sbt** : the sbt build script

== The sbt build script

Cloudflow provides sbt plugins for the supported runtimes, Akka, Spark and Flink. 
The plugins speed up development by adding the necessary dependencies and abstracting much of the boilerplate necessary to build a complete application. 
You can use multiple runtimes in the same application, provided that each runtime is defined in its own sub-project.
// TODO: Add here a link to the multi-project support

In this example, we use the `CloudflowAkkaPlugin` that provides the building blocks for developing a Cloudflow application with Akka Streams.
In addition to the backend-specific plugin, we need to add the `CloudflowApplicationPlugin` that provides the image-building and local-running capabilities to the project.

. Create a `build.sbt` file with the following contents and save it in at the same level as your `src` directory:
+
[source,scala]
----
include::{cloudflow-examples-version}@docsnippets:ROOT:example$sensor-data-scala/build.sbt[tag=get-started]
----
+
The script is a standard Scala sbt build file--with the addition of the Cloudflow plugin for Akka Streams, the Cloudflow application plugin, and also the Scalafmt plugin that we suggest to keep the style consistent in the application (optional).
+
. Create the file `project/cloudflow-plugins.sbt` and add the Cloudflow plugin dependency in it:
+
[source,scala]
----
include::{cloudflow-examples-version}@docsnippets:ROOT:example$sensor-data-scala/project/cloudflow-plugins.sbt[tag=get-started]
----

== Local configuration

To be able to run the example locally you should provide the two resources files mentioned in **build.sbt**.

. Create a `log4j.xml` file with the following contents and save it in your `src/main/resources/` directory:
+
[source,xml]
----
include::{cloudflow-examples-version}@docsnippets:ROOT:example$sensor-data-scala/src/main/resources/log4j.xml[]
----
+
you can tune the logging verbosity by changing the level(e.g. to `INFO`):
+
[source,xml]
----
<level value="INFO" />
----
+
. Create a `local.conf` file with the following contents and save it, as well, in your `src/main/resources/` directory:
+
[source,HOCON]
----
include::{cloudflow-examples-version}@docsnippets:ROOT:example$sensor-data-scala/src/main/resources/local.conf[]
----


== What's next

Now, let's xref:define-avro-schema.adoc[define the Avro schema].
