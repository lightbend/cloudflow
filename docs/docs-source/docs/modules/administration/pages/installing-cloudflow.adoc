= Installing Cloudflow
:toc:
:toc-title: ON THIS PAGE
:toclevels: 2

include::ROOT:partial$include.adoc[]

This guide shows how to install Cloudflow, using the `Helm` and `kubectl` command-line tools.
It also shows how to install Kafka, Spark, and Flink operators that integrate with Cloudflow.

== Prerequisites

=== CLI Tools

Make sure you have the following prerequisites installed before continuing:

- Helm, version 3 or later (check with `helm version`)
- Kubectl 

Before proceeding, make sure that `kubectl` is correctly installed and access the Kubernetes cluster to install Cloudflow. This can be done with the following command:

    kubectl version

=== Kafka

Kafka is used by Cloudflow to store data streams. Before installing Cloudflow, you need to make sure there is a Kafka cluster available and have the necessary broker bootstrap configuration.

The Kafka broker bootstrap configuration string is a comma-separated list of host/port pairs used by Cloudflow to establish the connection to the Kafka cluster. The configuration string should have the following format:

broker-1-address:broker-1-port, broker-2-address;broker-2-port

If you want to test Cloudflow and need a Kafka cluster, we recommend using Strimzi, a third-party Kafka operator that can create and manage Kafka clusters. 

The following chapter is a guide on how to configure and install a Kafka cluster using Strimzi: 

. xref:how-to-install-and-use-strimzi.adoc[Installing Kafka with Strimzi]

=== Storage requirements

If you plan to write Cloudflow applications using Spark or Flink, the Kubernetes cluster will need to have a storage class of the `ReadWriteMany` type installed. The name of the `ReadWriteMany` storage class name is required when installing Cloudflow.

For testing purposes, we suggest using the NFS Server Provisioner, which can be found here: https://github.com/helm/charts/tree/master/stable/nfs-server-provisioner[NFS Server Provisioner Helm chart]

We'll install the nfs chart in the `cloudflow` namespace, so create the namespace first:

  kubectl create ns cloudflow


Add the `Stable` Helm repository:

  helm repo add stable https://kubernetes-charts.storage.googleapis.com/

Update the Helm repository:

  helm repo update

Install the NFS Server Provisioner using the following command:

IMPORTANT: Depending on your Kubernetes configuration, you may want to adjust the values used during the install. 
Please see https://github.com/helm/charts/tree/master/stable/nfs-server-provisioner#configuration[NFS Server Provisioner configuration options].

  helm upgrade nfs-server-provisioner stable/nfs-server-provisioner \
    --install \
    --set storageClass.provisionerName=cloudflow-nfs \
    --namespace cloudflow

The result of the installation is shown below, the NFS Server provisioner pod is running and the new storage class exists.

----
$ kubectl get pods -n cloudflow
NAME                       READY   STATUS    RESTARTS   AGE
nfs-server-provisioner-0   1/1     Running   0          25s

$ kubectl get sc
NAME                 PROVISIONER            AGE
nfs                  cloudflow-nfs          29s
standard (default)   kubernetes.io/gce-pd   2m57s
----

NOTE: When installing Cloudflow, we will use the name `nfs` to indicate that Cloudflow should use the NFS storage class.

== Installing Cloudflow

In this guide, we will use Helm to install Cloudflow.

The first step is to create the namespace, if it does not exist yet, to install Cloudflow into:

  kubectl create ns cloudflow

IMPORTANT: Many subsequent commands will assume that the namespace is `cloudflow`.

First, we add the Cloudflow Helm repository:

  helm repo add cloudflow-helm-charts https://lightbend.github.io/cloudflow-helm-charts/

The `repo update` command ensures Helm has an up-to-date index of all Helm charts.

  helm repo update

Before installing Cloudflow, let's go over the custom parameters we are supplying with the command:

  cloudflow_operator.persistentStorageClass=nfs

The `persistentStorageClass` parameter sets the storage class to use with Spark and Flink applications. This storage class, as previously mentioned, has to be of the type `ReadWriteMany`. In our example, we are using the `nfs` storage class.

  cloudflow_operator.kafkaBootstrapservers=cloudflow-strimzi-kafka-bootstrap.cloudflow:9092

The `kafkaBootstrapservers` parameter sets the address and port of the Kafka cluster that Cloudflow will use. In this example, we have used the address of a Strimzi created Kafka cluster located in the `cloudflow` namespace.

The following command installs Cloudflow using the Helm chart:

  helm install cloudflow cloudflow-helm-charts/cloudflow --namespace cloudflow --set \
  strimzi.enabled=true --set cloudflow_operator.persistentStorageClass=nfs --set cloudflow_operator.kafkaBootstrapservers=cloudflow-strimzi-kafka-bootstrap.cloudflow:9092

Check the status of the install process using `kubectl`. When the Cloudflow operator pod is in `Running` status, the installation is complete.

----
$ kubectl get pods -n cloudflow
NAME                                                READY   STATUS    RESTARTS   AGE
cloudflow-operator-6b7d7cbdfc-xb6w5                 1/1     Running   0          10s
----

You can now deploy an Akka-based Cloudflow application into the cluster as it only requires Kafka to be set up. More on this in the development section of the documentation.

== Adding Spark support

If you plan to write applications that utilize Spark, you will need to install the Spark operator before deploying your Cloudflow application. As we can see below, the CLI will not allow you to deploy an application that does not have all dependencies installed:

  $ kubectl cloudflow deploy cloudflow/examples/call-record-aggregator/target/call-record-aggregator.json
  [Error] cannot detect that 'Spark' is installed, please install 'Spark' before continuing (exit status 1)

The following chapter is a guide on how to configure and install the Spark Operator: 

. xref:installing-spark-operator.adoc[Adding Spark Support]

== Adding Flink support

If you plan to write applications that utilize Flink you will need to install the Flink operator before deploying your Cloudflow application. As we can see below, the CLI will not allow you to deploy an application that does not have all dependencies installed:

  $ kubectl cloudflow deploy cloudflow/examples/taxi-ride/target/taxi-ride-fare.json
  [Error] cannot detect that Flink is installed, please install Flink before continuing (exit status 1)

The following chapter is a guide on how to configure and install the Spark Operator: 

. xref:installing-flink-operator.adoc[Adding Flink Support]

== Installing Enterprise components
If you have a commercial agreement with Lightbend, you are eligible to install the Cloudflow Enterprise components. 

Before installing, you need to make sure you have the credentials required to download the Helm chart from the Lightbend commercial repository.

Add the Lightbend Helm repository.

  helm repo add lightbend https://repo.lightbend.com/helm-charts/

Update Helm to make sure all charts are up-to-date

  helm repo update

Install the components, note that you set the `$USERNAME` and `$PASSWORD` environment variables before executing this command. 

We recommend using the following command to create an environment variable for username and password. 
*This is to avoid storing this information in the shell's history file.*

  read USERNAME
  read -s PASSWORD

TIP: If you want to see the password typed in, remove the `-s` (for silent) flag.

Once configured, we can execute the Helm command to install the Cloudflow enterprise components. Note how we use the `$USERNAME` and `$PASSWORD` in the Helm command.
 
----
helm upgrade cloudflow-enterprise-components lightbend/cloudflow-enterprise-components \
  --install \
  --namespace cloudflow \
  --set enterpriseOperatorVersion=2.0.8 \
  --set enterprise-suite.imageCredentials.username="$USERNAME" \
  --set enterprise-suite.imageCredentials.password="$PASSWORD"
----

After we have used the username and password environment variables, we can clear them by closing the shell or executing the following commands:

  unset USERNAME
  unset PASSWORD

To quickly verify that the installation worked, you can open a proxy to the Lightbend console service and confirm that you can access the Lightbend console UI.

First, create the port-forward.

  kubectl port-forward -n cloudflow svc/console-server 5000:80

Then open the following URL in your browser:

  https://localhost:5000

You should now see the Lightbend console in your browser.

== Upgrading Cloudflow

If you need to upgrade Cloudflow to a newer version, the following chapter in the administration section will show you how:

. xref::administration:upgrading-cloudflow.adoc[upgrading Cloudflow].
