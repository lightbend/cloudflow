= Installing Cloudflow
:toc:
:toc-title: ON THIS PAGE
:toclevels: 2

include::ROOT:partial$include.adoc[]

This guide shows how to install Cloudflow, using the `Helm` and `kubectl` command-line tools.
It also shows how to install Kafka, Spark, and Flink operators that integrate with Cloudflow.

== Prerequisites

=== CLI Tools

Make sure you have the following prerequisites installed before continuing:

- Helm, version 3 or later (check with `helm version`)
- Kubectl 

Before proceeding, make sure that `kubectl` is correctly installed and access the Kubernetes cluster to install Cloudflow. This can be done with the following command:

    kubectl version

=== Kafka

Kafka is used by Cloudflow to store data streams. Before installing Cloudflow, you need to make sure there is a Kafka cluster available and have the necessary broker bootstrap configuration.

The Kafka broker bootstrap configuration string is a comma-separated list of host/port pairs used by Cloudflow to establish the connection to the Kafka cluster. The configuration string should have the following format:

    broker-1-address:broker-1-port, broker-2-address;broker-2-port

If you want to test Cloudflow and need a Kafka cluster, we recommend using Strimzi, a third-party Kafka operator that can create and manage Kafka clusters. 

See xref:how-to-install-and-use-strimzi.adoc[Installing Kafka with Strimzi] as a guide on how to configure and install a Kafka cluster using Strimzi.

=== Storage requirements (for use with Spark or Flink)

**If you plan to write Cloudflow applications using Spark or Flink**, the Kubernetes cluster will need to have a storage class of the `ReadWriteMany` type installed. The name of the `ReadWriteMany` storage class name is required when installing Cloudflow.

For testing purposes, we suggest using the NFS Server Provisioner, which can be found here: https://github.com/helm/charts/tree/master/stable/nfs-server-provisioner[NFS Server Provisioner Helm chart]

We'll install the nfs chart in the `cloudflow` namespace, if it does not exist yet, create the `cloudflow` namespace:

  kubectl create ns cloudflow


Add the `Stable` Helm repository and update the local index:

  helm repo add stable https://kubernetes-charts.storage.googleapis.com/
  helm repo update

Install the NFS Server Provisioner using the following command:

IMPORTANT: Depending on your Kubernetes configuration, you may want to adjust the values used during the install. 
Please see https://github.com/helm/charts/tree/master/stable/nfs-server-provisioner#configuration[NFS Server Provisioner configuration options].

  helm upgrade nfs-server-provisioner stable/nfs-server-provisioner \
    --install \
    --set storageClass.provisionerName=cloudflow-nfs \
    --namespace cloudflow

The result of the installation is shown below, the NFS Server provisioner pod is running and the new storage class exists.

----
$ kubectl get pods -n cloudflow
NAME                       READY   STATUS    RESTARTS   AGE
nfs-server-provisioner-0   1/1     Running   0          25s

$ kubectl get sc
NAME                 PROVISIONER            AGE
nfs                  cloudflow-nfs          29s
standard (default)   kubernetes.io/gce-pd   2m57s
----

NOTE: When installing Cloudflow, we will use the name `nfs` to indicate that Cloudflow should use the NFS storage class.

== Installing Cloudflow

In this guide, we will use Helm to install Cloudflow.

The first step is to create the namespace, if it does not exist yet, to install Cloudflow into:

  kubectl create ns cloudflow

IMPORTANT: Many subsequent commands will assume that the namespace is `cloudflow`.

First, we add the Cloudflow Helm repository and update the local index:

  helm repo add cloudflow-helm-charts https://lightbend.github.io/cloudflow-helm-charts/
  helm repo update

=== Installing (with support for Spark or Flink)

For use with Spark or Flink, the `persistentStorageClass` parameter sets the storage class to `nfs`. This storage class, as mentioned in <<_storage_requirements>>, has to be of the type `ReadWriteMany`. In our example, we are using the `nfs` storage class.

  cloudflow_operator.persistentStorageClass=nfs

The `kafkaBootstrapservers` parameter sets the address and port of the Kafka cluster that Cloudflow will use. In this example, we have used the address of a Strimzi created Kafka cluster located in the `cloudflow` namespace.

  cloudflow_operator.kafkaBootstrapservers=cloudflow-strimzi-kafka-bootstrap.cloudflow:9092

The following command installs Cloudflow using the Helm chart:

  helm install cloudflow cloudflow-helm-charts/cloudflow --namespace cloudflow \
    --set strimzi.enabled=true \
    --set cloudflow_operator.persistentStorageClass=nfs \
    --set cloudflow_operator.kafkaBootstrapservers=cloudflow-strimzi-kafka-bootstrap.cloudflow:9092


=== Installing (no support for Spark or Flink)

The `kafkaBootstrapservers` parameter sets the address and port of the Kafka cluster that Cloudflow will use. In this example, we have used the address of a Strimzi created Kafka cluster located in the `cloudflow` namespace.

  cloudflow_operator.kafkaBootstrapservers=cloudflow-strimzi-kafka-bootstrap.cloudflow:9092

The following command installs Cloudflow using the Helm chart:

  helm install cloudflow cloudflow-helm-charts/cloudflow --namespace cloudflow \
    --set strimzi.enabled=true \
    --set cloudflow_operator.kafkaBootstrapservers=cloudflow-strimzi-kafka-bootstrap.cloudflow:9092

=== Verifying the installation

Check the status of the installation process using `kubectl`. When the Cloudflow operator pod is in `Running` status, the installation is complete.

----
$ kubectl get pods -n cloudflow
NAME                                                READY   STATUS    RESTARTS   AGE
cloudflow-operator-6b7d7cbdfc-xb6w5                 1/1     Running   0          10s
----

You can now deploy an Akka-based Cloudflow application into the cluster as it only requires Kafka to be set up. More on this in the development section of the documentation.

== Adding Spark support

If you plan to write applications that utilize Spark, you will need to install the Spark operator before deploying your Cloudflow application. As we can see below, the CLI will not allow you to deploy an application that does not have all dependencies installed:

  $ kubectl cloudflow deploy cloudflow/examples/call-record-aggregator/target/call-record-aggregator.json
  [Error] cannot detect that 'Spark' is installed, please install 'Spark' before continuing (exit status 1)

To install the Spark Operator see xref:installing-spark-operator.adoc[Adding Spark Support].

== Adding Flink support

If you plan to write applications that utilize Flink you will need to install the Flink operator before deploying your Cloudflow application. As we can see below, the CLI will not allow you to deploy an application that does not have all dependencies installed:

  $ kubectl cloudflow deploy cloudflow/examples/taxi-ride/target/taxi-ride-fare.json
  [Error] cannot detect that Flink is installed, please install Flink before continuing (exit status 1)

To install the Flink Operator see xref:installing-flink-operator.adoc[Adding Flink Support].

== Installing Enterprise components
If you have a commercial agreement with Lightbend, you are eligible to install the Cloudflow Enterprise components. 

Before installing, you need to make sure you have the credentials required to download the Helm chart from the Lightbend commercial repository.

Add the Lightbend Helm repository and update the local index:

  helm repo add lightbend https://repo.lightbend.com/helm-charts/
  helm repo update

Install the components, note that you set the `$USERNAME` and `$PASSWORD` environment variables before executing this command. 

We recommend using the following command to create an environment variable for username and password. 
*This is to avoid storing this information in the shell's history file.*

  read USERNAME
  read -s PASSWORD

TIP: If you want to see the password typed in, remove the `-s` (for silent) flag.

Once configured, we can execute the Helm command to install the Cloudflow enterprise components. Note how we use the `$USERNAME` and `$PASSWORD` in the Helm command.

[source,shell script,subs="attributes+"]
----
helm upgrade cloudflow-enterprise-components lightbend/cloudflow-enterprise-components \
  --install \
  --namespace cloudflow \
  --set enterpriseOperatorVersion={cloudflow-version} \
  --set enterprise-suite.imageCredentials.username="$USERNAME" \
  --set enterprise-suite.imageCredentials.password="$PASSWORD"
----

After we have used the username and password environment variables, we can clear them by closing the shell or executing the following commands:

  unset USERNAME
  unset PASSWORD

To quickly verify that the installation worked, you can open a proxy to the Lightbend console service and confirm that you can access the Lightbend console UI.

First, create the port-forward.

  kubectl port-forward -n cloudflow svc/console-server 5000:80

Then open the following URL in your browser:

  https://localhost:5000

You should now see the Lightbend console in your browser.

== Upgrading Cloudflow

If you need to upgrade Cloudflow to a newer version, xref:upgrading-cloudflow.adoc[upgrading Cloudflow] in the administration section will show you how.
