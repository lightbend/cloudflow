:page-partial:

include::ROOT:partial$include.adoc[]

// Moved to developing streamlets
Please read xref:ROOT:index.adoc[the introduction] to get an overview of Cloudflow. This section goes into more depth on streamlets and how to develop them. Akka and Flink streamlets can be developed in Scala or Java, Spark streamlets can only be developed in Scala.

[[message-delivery-semantics]]
== Message delivery semantics

Cloudflow follows the 'let it crash' principle and can recover from most failure scenarios, except those deemed catastrophic, where the data used for recovery (snapshots) may have been lost.
This approach also follows the general policy of Kubernetes, where processes are ephemeral and can be restarted, replaced, or scaled up/down at any time.

The sections that follow mention different models for message delivery semantics provided by Spark-based and Akka-based streamlets. By _message delivery semantics_, we refer to the expected message delivery guaranties in the case of failure recovery. In a distributed application such as Cloudflow, failure may happen at several different execution levels: from a failed task in an individual executor, to a pod that goes down, to a complete application crash.

After a failure recovery, we recognize these different message delivery guarantees:

_At most once_:: Data may have been processed but will never be processed twice. In this case, data may be lost but processing will never result in duplicate records.
_At-least-once_:: Data that has been processed may be replayed and processed again. In this case, each data record is guaranteed to be processed and may result in duplicate records.
_Exactly once_:: Data is processed once and only once. All data is guaranteed to be processed and no duplicate records are generated. This is the most desirable guarantee for many enterprise applications, but it's considered impossible to achieve in a distributed environment.
_Effectively Exactly Once_:: is a variant of _exactly once_ delivery semantics that tolerates duplicates during data processing and requires the producer side of the process to be idempotent.
That is, producing the same record more than once is the same as producing it only once.
In practical terms, this translates to writing the data to a system that can preserve the uniqueness of keys or use a deduplication process to prevent duplicate records from being produced to an external system.

== Streamlet configuration parameters

A streamlet can require dynamic configuration parameters at deployment time. Configuration parameters can be used to change the way the streamlet functions when it is run.

Examples of configuration parameters are database connection strings, URLs, credentials, or anything else that you want to specify at deployment time.

A streamlet specifies that it requires particular config parameters by expressing them in code. The values for these parameters will be requested, validated, and set when `kubectl cloudflow deploy` is used to deploy the Cloudflow application.

There are a number of predefined configuration parameter types:

|======
|`IntegerConfigParameter`| A signed 32 bit integer value.
|`StringConfigParameter`| A string with the max length of 1k characters.
|`DoubleConfigParameter`| A 64 bit floating point value.
|`BooleanConfigParameter`| A boolean value.
|`RegExpConfigParameter`| A string validated using a regular expression.
|`DurationConfigParameter`| A duration string, for example "2 minutes".
|`MemorySizeConfigParameter`| A memory size string, for example "32M".
|======

In addition to the predefined types, you can also define your own types.

=== Using a configuration parameter in a streamlet

The following section will break down how we can use an `Integer` configuration parameter type in a streamlet to request the value for a maximum number of records within a time window.

[source,scala]
--
include::{cloudflow-examples-version}@docsnippets:ROOT:example$akkastreams-scala/src/main/scala/cloudflow/akkastreamsdoc/RecordSumFlow.scala[tag=all]
--

As seen in the example below, we first need to create an instance of `IntegerConfigParameter`.

[source,scala]
--
include::{cloudflow-examples-version}@docsnippets:ROOT:example$akkastreams-scala/src/main/scala/cloudflow/akkastreamsdoc/RecordSumFlow.scala[tag=definition]
--

The arguments provided to `IntegerConfigParameter()` are the following:

    - A key, which has to be unique within the streamlet.
    - Optionally, a description, which will be shown by the CLI.
    - Optionally, a default value, which will be used by the CLI when no value is passed during deploy.

After the configuration parameter is defined, we can use it to extract its value from the runtime configuration in the `createLogic` function:

[source,scala]
--
include::{cloudflow-examples-version}@docsnippets:ROOT:example$akkastreams-scala/src/main/scala/cloudflow/akkastreamsdoc/RecordSumFlow.scala[tag=usage]
--

Note that its up to the developer to use the correct config method to extract the value of the parameter. Since the type being used here is `IntegerConfigParameter` the config method used is `getInt`.

=== Custom validation

It is easy to create your own custom validation for a configuration parameter using the `RegExpConfigParameter` type. This type allows you to validate the entered value using a regular expression.

For example, if we want to validate a 24 hour timestamp, this is how it could be defined and used in a streamlet.

[source,scala]
--
include::{cloudflow-examples-version}@docsnippets:ROOT:example$akkastreams-scala/src/main/scala/cloudflow/akkastreamsdoc/ConfigCustomValidation.scala[tag=definition]
--

=== Providing values for configuration parameters when testing streamlets

When writing tests for streamlets, you can provide values for configuration parameters when you initialize the runner-specific testkit.

If we want to write a test for the example streamlet `RecordSumFlow`, we could add values for the `recordsInWindowParameter` configuration parameter like this:

[source,scala]
--
include::{cloudflow-examples-version}@docsnippets:ROOT:example$akkastreams-scala/src/test/scala/com/example/SampleSpec.scala[tag=config-value]
--

The Spark testkit has a similar function for adding values to configuration parameters when testing a streamlet.

[source,scala]
--
include::{cloudflow-examples-version}@docsnippets:ROOT:example$spark-scala/src/test/scala/com/example/SampleSpec.scala[tag=config-value]
--

The Java API is slightly different as you can see in the example below:

[source,java]
--
include::{cloudflow-examples-version}@docsnippets:ROOT:example$akkastreams-java/src/test/java/com/example/SampleTest.java[tag=config-value]
--

=== Using configuration parameters in Java

Using the Configuration parameters in Java is similar to the Scala version. The main difference is how class instantiation is done and how to retrieve the config parameter key.

Creating an instance of a `StringConfigParameter` in Java:

[source,java]
--
include::{cloudflow-examples-version}@docsnippets:ROOT:example$akkastreams-java/src/main/java/cloudflow/akkastreamsdoc/FilterStreamlet.java[tag=definition]
--

Example of accessing the value of a configuration parameter in Java:

[source,java]
----
include::{cloudflow-examples-version}@docsnippets:ROOT:example$akkastreams-java/src/main/java/cloudflow/akkastreamsdoc/FilterStreamlet.java[tag=usage]
----


=== Providing configuration parameters when deploying a Cloudflow application

Configuration parameters will need to be provided with values during deployment of the application. The `deploy` command accepts these values as a set of key/value pairs.

The format for specifying configuration parameter values is as follows:

    [streamlet name].[configuration parameter name]="[value]"

Deploying an application without specifying values for all required configuration parameters will fail and result in an error message like the following.

NOTE: Examples in this section show the OpenShift `oc plugin`, however you should substitute the correct command for your K8s distribution.

[source,bash,subs=attributes+]
----
$ {cli-plugin} cloudflow deploy registry-default.my.kubernetes.cluster/cloudflow/call-record-pipeline:292-c183d80 cdr-aggregator.group-by-window="7 minute" cdr-aggregator.watermark="1 minute"

[Error] Please provide values for the following configuration parameter(s):
- cdr-generator2.records-per-second - Records per second to process.
- cdr-generator1.records-per-second - Records per second to process.
----

To successfully deploy the application, all configuration parameter values have to be provided on the command line:

[source,bash,subs=attributes+]
----
$ {cli-plugin} cloudflow deploy registry-default.my.kubernetes.cluster/cloudflow/call-record-pipeline:292-c183d80 cdr-aggregator.group-by-window="7 minute" cdr-aggregator.watermark="1 minute" cdr-generator1.records-per-second="10" cdr-generator2.records-per-second="10"

[Done] Deployment of application `call-record-aggregator` has started.
----

Configuration parameters can be omitted from the deployment command line as long as they have default values. 
[source,bash,subs=attributes+]
----
$ {cli-plugin} cloudflow deploy registry-default.my.kubernetes.cluster/cloudflow/sensor-data-java:439-a5837b5

Default value 'device-ids.txt' will be used for configuration parameter 'filter.filter-filename'
Default value '10' will be used for configuration parameter 'filter.filter-pollinginterval'

[Done] Deployment of application `sensor-data-java` has started.
----


== Streamlet volume mounts

Sometimes a streamlet needs to read and/or write files from/to some shared file system. Since streamlets run as processes on Kubernetes, they do not automatically have such a file system available. Cloudflow makes it possible for a streamlet to declare the need for a shared file system (e.g. a "volume" in Kubernetes terms) that should be mounted at a specific path. At deployment time the user can then indicate where that file system is actually located using a Kubernetes Persistent Volume Claim (PVC). Cloudflow will then make sure that the PVC will be mounted at the specified path at runtime and the streamlet can then treat it like a local file system.

The following example streamlet shows how to declare and use a volume mount:

[source,scala]
----
include::{cloudflow-examples-version}@docsnippets:ROOT:example$akkastreams-scala/src/main/scala/cloudflow/akkastreamsdoc/DataFileIngress.scala[]
----

=== Java API

The Java API is slightly different from the Scala API. The example belows shows a streamlet that uses a read only volume mount.


[source,java]
----
include::{cloudflow-examples-version}@docsnippets:ROOT:example$akkastreams-java/src/main/java/cloudflow/akkastreamsdoc/FilterStreamlet.java[]
----


If you want to use a writable volume mount you can replace `createReadOnlyMany` with `createReadWriteMany` above.

=== Access Modes and PVC Mounting

The PVC associated with the streamlet volume mount is required to have the same access mode as the volume mount declared in the streamlet. When deploying the application the access mode will be checked, if the access mode differs from the access mode declared in the streamlet, the deployment of the application will fail.

The following access modes are available:

- `ReadOnlyMany`: all streamlet instances get read-only access to the same volume.
- `ReadWriteMany`: all streamlet instances get read and write access to the same volume.

== Cluster Security Considerations

When deploying a Cloudflow application that contains streamlets with a volume mount, you may have to apply additional Kubernetes security configuration resources to the Kubernetes cluster for the application to deploy successfully.

The pod in which the streamlet is running may need to be associated with a Pod Security Context (PSP) or (on OpenShift) a Security Context Constraint (SCC).

This can be done by associating the Cloudflow application service account, called `cloudflow-app-serviceaccount` and located in the namespace of the application, with a PSP/SCC.

The PSP and SCC must allow the application pods to mount a writable volume as group id `185`. This is the group id of the user running in the streamlet container.

=== Security context constraints example

The following example shows an SCC that would allow a Cloudflow application with a writable volume mount to deploy correctly to an Openshift cluster with an activated SCC controller. See the OpenShift documentation on https://docs.openshift.com/container-platform/3.11/admin_guide/manage_scc.html[Managing Security Context Constraints] for more information.

[source,yaml]
----
kind: SecurityContextConstraints
apiVersion: v1
metadata:
  name: cloudflow-application-scc
allowPrivilegedContainer: true
runAsUser:
  type: MustRunAsNonRoot
seLinuxContext:
  type: RunAsAny
fsGroup:
  type: MustRunAs
  ranges:
  - min: 185
    max: 186
supplementalGroups:
  type: RunAsAny
volumes:
- '*'
----


=== Pod security policy example

This is an example of a PSP that would allow a Cloudflow application with a writable volume mount to deploy correctly.

[source,yaml]
----
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: cloudflow-volume-mount-psp
spec:
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'MustRunAs'
    ranges:
    - min: 185
      max: 186
  volumes:
  - '*'
----

https://kubernetes.io/docs/concepts/policy/pod-security-policy/[Pod Security Policies]

== Deploying applications using volume mounts

When deploying a Cloudflow application with streamlets that use the volume mount feature, a Kubernetes Persistent Volume Claim (PVC) will need to be specified for each of the volume mounts. 

Before the application can be deployed, the PVC needs to be created in the application namespace.

When the PVC has been created, you can deploy the application and associate the PVC with the streamlet volume mount name using a CLI flag.

Deploying an application without a required volume mount will fail and result in an error message:

[source,bash,subs=attributes+]
----
$ {cli-plugin} cloudflow deploy sensor-data-java:427-a20fc62-dirty

[Error] The following volume mount needs to be bound to a Persistence Volume claim using the --volume-mount flag

- filter.configuration
----

To successfully deploy the application, the volume mount has to be bound to a PVC.

In the example below, the streamlet `filter` requires a volume mount called `configuration`. This volume mount is associated with the PVC named `source-data-claim` using the `--volume-mount` flag.

[source,bash,subs=attributes+]
----
$ {cli-plugin} cloudflow deploy sensor-data-java:427-a20fc62-dirty --volume-mount filter.configuration=source-data-claim 

The following volume mount is now bound to Persistent Volume Claim `source-data-claim`:

- filter.configuration

[Done] Deployment of application `sensor-data-java` has started.
----

== What's next

After this general overview of streamlets, learn more about the specifics of using xref:use-akka-streamlets.adoc[Akka], xref:use-spark-streamlets.adoc[Spark], and xref:use-flink-streamlets.adoc[Flink] streamlets. Cloudflow also offers a way to test your streamlets in a xref:cloudflow-local-sandbox.adoc[local sandbox].

